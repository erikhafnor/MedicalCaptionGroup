{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":10152502,"datasetId":6267882,"databundleVersionId":10438680},{"sourceType":"kernelVersion","sourceId":236410398}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-deps bert-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:32.623219Z","iopub.execute_input":"2025-04-28T11:38:32.623720Z","iopub.status.idle":"2025-04-28T11:38:35.052666Z","shell.execute_reply.started":"2025-04-28T11:38:32.623696Z","shell.execute_reply":"2025-04-28T11:38:35.051797Z"}},"outputs":[{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#IMPORTS\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport math\nimport json\nfrom collections import Counter\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom bert_score import score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:35.054262Z","iopub.execute_input":"2025-04-28T11:38:35.054499Z","iopub.status.idle":"2025-04-28T11:38:45.457889Z","shell.execute_reply.started":"2025-04-28T11:38:35.054481Z","shell.execute_reply":"2025-04-28T11:38:45.457306Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# reading training captions\nfile_path = '/kaggle/input/rocov2/ROCOv2/train_captions.csv'\ndata = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.458533Z","iopub.execute_input":"2025-04-28T11:38:45.458898Z","iopub.status.idle":"2025-04-28T11:38:45.785702Z","shell.execute_reply.started":"2025-04-28T11:38:45.458878Z","shell.execute_reply":"2025-04-28T11:38:45.784902Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# reading testing captions\nfile_path_test = '/kaggle/input/rocov2/ROCOv2/test_captions.csv'\ndata_test = pd.read_csv(file_path_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.787912Z","iopub.execute_input":"2025-04-28T11:38:45.788180Z","iopub.status.idle":"2025-04-28T11:38:45.841297Z","shell.execute_reply.started":"2025-04-28T11:38:45.788161Z","shell.execute_reply":"2025-04-28T11:38:45.840607Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.842097Z","iopub.execute_input":"2025-04-28T11:38:45.842375Z","iopub.status.idle":"2025-04-28T11:38:45.861638Z","shell.execute_reply.started":"2025-04-28T11:38:45.842350Z","shell.execute_reply":"2025-04-28T11:38:45.860950Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                         ID                                            Caption\n0  ROCOv2_2023_train_000001            Head CT demonstrating left parotiditis.\n1  ROCOv2_2023_train_000002  Acquired renal cysts in end-stage renal failur...\n2  ROCOv2_2023_train_000003  Computed tomography of the chest showing the r...\n3  ROCOv2_2023_train_000004  Lateral view of the sacrum showing the low con...\n4  ROCOv2_2023_train_000005  Thoracic CT scan showing perihilar pulmonary l...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ROCOv2_2023_train_000001</td>\n      <td>Head CT demonstrating left parotiditis.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ROCOv2_2023_train_000002</td>\n      <td>Acquired renal cysts in end-stage renal failur...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ROCOv2_2023_train_000003</td>\n      <td>Computed tomography of the chest showing the r...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ROCOv2_2023_train_000004</td>\n      <td>Lateral view of the sacrum showing the low con...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ROCOv2_2023_train_000005</td>\n      <td>Thoracic CT scan showing perihilar pulmonary l...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"data_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.862260Z","iopub.execute_input":"2025-04-28T11:38:45.862427Z","iopub.status.idle":"2025-04-28T11:38:45.868986Z","shell.execute_reply.started":"2025-04-28T11:38:45.862414Z","shell.execute_reply":"2025-04-28T11:38:45.868236Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                        ID                                            Caption\n0  ROCOv2_2023_test_000001  CT chest axial view showing a huge ascending a...\n1  ROCOv2_2023_test_000002  Computed tomography (CT) shows floating thromb...\n2  ROCOv2_2023_test_000003  Digitally subtracted angiogram demonstrates ac...\n3  ROCOv2_2023_test_000004  Digitally subtracted angiogram of the IMA demo...\n4  ROCOv2_2023_test_000005               Angle measurement of a Type 1 canal.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ROCOv2_2023_test_000001</td>\n      <td>CT chest axial view showing a huge ascending a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ROCOv2_2023_test_000002</td>\n      <td>Computed tomography (CT) shows floating thromb...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ROCOv2_2023_test_000003</td>\n      <td>Digitally subtracted angiogram demonstrates ac...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ROCOv2_2023_test_000004</td>\n      <td>Digitally subtracted angiogram of the IMA demo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ROCOv2_2023_test_000005</td>\n      <td>Angle measurement of a Type 1 canal.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"captions = data['Caption'].tolist()\ncaptions = captions[:10000]\nprint(captions[:5])\nprint(len(captions))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.869734Z","iopub.execute_input":"2025-04-28T11:38:45.870449Z","iopub.status.idle":"2025-04-28T11:38:45.886332Z","shell.execute_reply.started":"2025-04-28T11:38:45.870422Z","shell.execute_reply":"2025-04-28T11:38:45.885806Z"}},"outputs":[{"name":"stdout","text":"['Head CT demonstrating left parotiditis.', 'Acquired renal cysts in end-stage renal failure: 16-year-old girl with Alport syndrome and peritoneal dialysis from the age of 2\\xa0years', 'Computed tomography of the chest showing the right breast nodule with irregular margins', 'Lateral view of the sacrum showing the low contrast between bone and soft tissue.', 'Thoracic CT scan showing perihilar pulmonary lymphadenomegaly']\n10000\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"captions_test = data_test['Caption'].tolist()\ncaptions_test = captions_test[:2000]\nprint(captions_test[:5])\nprint(len(captions_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.886947Z","iopub.execute_input":"2025-04-28T11:38:45.887115Z","iopub.status.idle":"2025-04-28T11:38:45.898061Z","shell.execute_reply.started":"2025-04-28T11:38:45.887102Z","shell.execute_reply":"2025-04-28T11:38:45.897436Z"}},"outputs":[{"name":"stdout","text":"['CT chest axial view showing a huge ascending aortic aneurysm (*).', 'Computed tomography (CT) shows floating thrombosis (white arrow)', 'Digitally subtracted angiogram demonstrates active extravasation of the superior rectal artery into the ileal-conduit (blue arrow)', 'Digitally subtracted angiogram of the IMA demonstrated cessation of flow through the proximal superior rectal artery in the region of the intersection between the artery and ureter with retained perfusion of the rectosigmoid region and resolution of active extravasation', 'Angle measurement of a Type 1 canal.']\n2000\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"base_path = \"/kaggle/input/rocov2/ROCOv2/train_images/train/\"\nimage_ids = data['ID']\nimage_paths = [f\"{base_path}{img_id}.jpg\" for img_id in image_ids]\nimages = [Image.open(path) for path in image_paths]\nimages = images[:24000]\n\nprint(images[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:38:45.898933Z","iopub.execute_input":"2025-04-28T11:38:45.899153Z","iopub.status.idle":"2025-04-28T11:44:46.475266Z","shell.execute_reply.started":"2025-04-28T11:38:45.899138Z","shell.execute_reply":"2025-04-28T11:44:46.474486Z"}},"outputs":[{"name":"stdout","text":"[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=682x748 at 0x78275F131210>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=307x224 at 0x78276016CFD0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=358x263 at 0x78275F131A50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=567x567 at 0x7828707381D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x512 at 0x78275EAC3610>]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"base_path_test = \"/kaggle/input/rocov2/ROCOv2/test_images/test/\"\nimage_ids_test = data_test['ID']\nimage_paths_test = [f\"{base_path_test}{img_id}.jpg\" for img_id in image_ids_test]\nimages_test = [Image.open(path) for path in image_paths_test]\nimages_test = images_test[:6000]\n\nprint(images_test[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:44:46.477841Z","iopub.execute_input":"2025-04-28T11:44:46.478077Z","iopub.status.idle":"2025-04-28T11:45:53.665690Z","shell.execute_reply.started":"2025-04-28T11:44:46.478043Z","shell.execute_reply":"2025-04-28T11:45:53.664931Z"}},"outputs":[{"name":"stdout","text":"[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=653x658 at 0x78275048C5D0>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=598x669 at 0x78275048C510>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=896x977 at 0x78275048C990>, <PIL.JpegImagePlugin.JpegImageFile image mode=L size=896x875 at 0x78275048C450>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=782x516 at 0x78275048D510>]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Load a pre-trained ResNet model and move it to the GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nresnet = models.resnet50(pretrained=True).to(device)\nresnet = nn.Sequential(*list(resnet.children())[:-1])   # remove the classification layer to fit transformer\nresnet.eval().to(device)\n\n# Define a transformation pipeline for the images\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Function to extract features from an image\ndef extract_features(image):\n    # Ensure the image is in RGB format\n    image = image.convert(\"RGB\")\n    \n    # Apply transformations\n    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n    \n    # Extract features using the model\n    with torch.no_grad():\n        features = resnet(image_tensor)\n    \n    return features.squeeze().cpu().numpy()  # Move back to CPU for further processing\n\n# Initialize lists to store features and captions\nall_features = []\nall_captions = []\n\n# Extract features for all images and log to wandb\nfor idx, (image, caption) in enumerate(zip(images, captions)):\n    features = extract_features(image)\n    all_features.append(features)\n    all_captions.append(caption)\n    \n# Save features and captions to local files\nnp.save(\"/kaggle/working/features_24000.npy\", np.array(all_features))  # Save features as a NumPy array\nwith open(\"/kaggle/working/captions_24000.json\", \"w\") as f:\n    json.dump(all_captions, f)  # Save captions as a JSON file\n\n# Initialize lists to store features and captions\nall_features_test = []\nall_captions_test = []\n\n# Extract features for all images and log to wandb\nfor idx, (image, caption) in enumerate(zip(images_test, captions_test)):\n    features_test = extract_features(image)\n    all_features_test.append(features_test)\n    all_captions_test.append(caption)\n    \n# Save features and captions to local files\nnp.save(\"/kaggle/working/features_6000_test.npy\", np.array(all_features_test))  # Save features as a NumPy array\nwith open(\"/kaggle/working/captions_6000_test.json\", \"w\") as f:\n    json.dump(all_captions_test, f)  # Save captions as a JSON file\n\nprint(\"Features and captions saved locally.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:45:53.666364Z","iopub.execute_input":"2025-04-28T11:45:53.666595Z","iopub.status.idle":"2025-04-28T11:48:46.864459Z","shell.execute_reply.started":"2025-04-28T11:45:53.666548Z","shell.execute_reply":"2025-04-28T11:48:46.863711Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 217MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Features and captions saved locally.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nfeatures = np.load(\"/kaggle/working/features_24000.npy\")\nfeatures = torch.tensor(features).to(device)  # Convert to tensor and move to GPU\nfeatures = features.float()\nprint(features.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:46.865359Z","iopub.execute_input":"2025-04-28T11:48:46.865682Z","iopub.status.idle":"2025-04-28T11:48:46.968028Z","shell.execute_reply.started":"2025-04-28T11:48:46.865653Z","shell.execute_reply":"2025-04-28T11:48:46.967404Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\ntorch.Size([10000, 2048])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"features_test = np.load(\"/kaggle/working/features_6000_test.npy\")\nfeatures_test = torch.tensor(features_test).to(device)  # Convert to tensor and move to GPU\nfeatures_test = features_test.float()\nprint(features_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:46.968984Z","iopub.execute_input":"2025-04-28T11:48:46.969243Z","iopub.status.idle":"2025-04-28T11:48:46.982339Z","shell.execute_reply.started":"2025-04-28T11:48:46.969215Z","shell.execute_reply":"2025-04-28T11:48:46.981641Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2000, 2048])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"with open(\"/kaggle/working/captions_24000.json\", \"r\") as f:\n    captions = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:46.983067Z","iopub.execute_input":"2025-04-28T11:48:46.983337Z","iopub.status.idle":"2025-04-28T11:48:46.993515Z","shell.execute_reply.started":"2025-04-28T11:48:46.983312Z","shell.execute_reply":"2025-04-28T11:48:46.992990Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"with open(\"/kaggle/working/captions_6000_test.json\", \"r\") as f:\n    captions_test = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:46.994414Z","iopub.execute_input":"2025-04-28T11:48:46.994686Z","iopub.status.idle":"2025-04-28T11:48:47.007813Z","shell.execute_reply.started":"2025-04-28T11:48:46.994665Z","shell.execute_reply":"2025-04-28T11:48:47.007083Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Create vocabulary\nvocab = []\nall_captions = captions + captions_test\nfor caption in all_captions:\n    words = caption.lower().split()\n    vocab.extend(words)\nword_count = Counter(vocab)\n\n# Initialize special tokens and their indices\nword2idx = {\"<pad>\":0, \"<start>\":1, \"<end>\":2, \"<unk>\":3}\n\n# Add and assign index to each word from vocabulary\nfor idx, word in enumerate(word_count.keys(), start=len(word2idx)):\n    word2idx[word] = idx\n\nvocab_size = len(word2idx)\nprint(vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.008890Z","iopub.execute_input":"2025-04-28T11:48:47.009282Z","iopub.status.idle":"2025-04-28T11:48:47.091591Z","shell.execute_reply.started":"2025-04-28T11:48:47.009263Z","shell.execute_reply":"2025-04-28T11:48:47.090970Z"}},"outputs":[{"name":"stdout","text":"22207\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Change each caption into list of word indices\ntoken_ids_list  = []\nfor caption in captions:\n    words = caption.lower().split()\n\n    token_ids = []\n    \n    token_ids.append(word2idx[\"<start>\"])\n\n    for word in words:\n        if word in word2idx:\n            token_ids.append(word2idx[word])\n        else:\n            token_ids.append(word2idx[\"<unk>\"])\n            \n    token_ids.append(word2idx[\"<end>\"])\n    \n    token_ids_list.append(token_ids)\n\nprint(token_ids_list[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.092418Z","iopub.execute_input":"2025-04-28T11:48:47.092643Z","iopub.status.idle":"2025-04-28T11:48:47.179154Z","shell.execute_reply.started":"2025-04-28T11:48:47.092628Z","shell.execute_reply":"2025-04-28T11:48:47.178612Z"}},"outputs":[{"name":"stdout","text":"[[1, 4, 5, 6, 7, 8, 2], [1, 9, 10, 11, 12, 13, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 2], [1, 29, 30, 26, 24, 31, 32, 24, 33, 34, 35, 17, 36, 37, 2], [1, 38, 39, 26, 24, 40, 32, 24, 41, 42, 43, 44, 20, 45, 46, 2], [1, 47, 5, 48, 32, 49, 50, 51, 2]]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Add padding\nmax_seq_len = max(len(seq) for seq in token_ids_list)\npadded_token_ids_list = []\nfor seq in token_ids_list:\n    padded_seq = seq + [word2idx[\"<pad>\"]] * (max_seq_len - len(seq))\n    padded_token_ids_list.append(padded_seq)\n\n# Convert padded sequenced to tensor\ntoken_ids = torch.tensor(padded_token_ids_list).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.179863Z","iopub.execute_input":"2025-04-28T11:48:47.180068Z","iopub.status.idle":"2025-04-28T11:48:47.482462Z","shell.execute_reply.started":"2025-04-28T11:48:47.180047Z","shell.execute_reply":"2025-04-28T11:48:47.481897Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Function giving information about the order of words\ndef positional_encoding(x, max_length = 5000):\n    batch_size, seq_len, embed_dim = x.size()\n    max_length = max(max_length, seq_len)\n    \n    pe_matrix = torch.zeros(max_length, embed_dim, device = x.device) # device = x.device avoids mixing tensors across CPU and GPU (Expected all tensors to be on the same device)\n\n    # Generate position indices\n    position = torch.arange(0, max_length, device = x.device).unsqueeze(1).float()\n    \n    # Dividing terms for sine (even idx) and cosine (odd idx)\n    dividing_terms = torch.exp(torch.arange(0, embed_dim, 2, device = x.device).float() * (-math.log(10000) / embed_dim))\n    pe_matrix[:, 0::2] = torch.sin(position * dividing_terms)\n    pe_matrix[:, 1::2] = torch.cos(position * dividing_terms)\n\n    # Add positional encoding to the input tensor\n    x = x + pe_matrix[:seq_len, :].unsqueeze(0)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.483237Z","iopub.execute_input":"2025-04-28T11:48:47.483518Z","iopub.status.idle":"2025-04-28T11:48:47.489700Z","shell.execute_reply.started":"2025-04-28T11:48:47.483496Z","shell.execute_reply":"2025-04-28T11:48:47.489069Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Parameters\nnum_epochs = 10\nlearning_rate = 0.0001\n\nembedding_size = 512\nattention_head_num = 8\nhidden_layers = 512\ndecoder_layers_num = 3\npad_token_id = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.490474Z","iopub.execute_input":"2025-04-28T11:48:47.490822Z","iopub.status.idle":"2025-04-28T11:48:47.501197Z","shell.execute_reply.started":"2025-04-28T11:48:47.490802Z","shell.execute_reply":"2025-04-28T11:48:47.500590Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Define model\nencoder_projection = nn.Linear(2048, embedding_size).to(device)\ncaption_embedding = nn.Embedding(vocab_size, embedding_size).to(device)\n\nembeddings = caption_embedding(token_ids)\n\ndecoder_layer = nn.TransformerDecoderLayer(d_model = embedding_size, nhead = attention_head_num, dim_feedforward = hidden_layers)\ntransformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers = decoder_layers_num)\n\noutput_layer = nn.Linear(embedding_size, vocab_size).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.502081Z","iopub.execute_input":"2025-04-28T11:48:47.502365Z","iopub.status.idle":"2025-04-28T11:48:47.783695Z","shell.execute_reply.started":"2025-04-28T11:48:47.502343Z","shell.execute_reply":"2025-04-28T11:48:47.782625Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index = pad_token_id)\noptimizer = optim.Adam(\n    list(encoder_projection.parameters()) +\n    list(caption_embedding.parameters()) +\n    list(transformer_decoder.parameters()) +\n    list(output_layer.parameters()),\n    lr = learning_rate\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.784449Z","iopub.execute_input":"2025-04-28T11:48:47.784696Z","iopub.status.idle":"2025-04-28T11:48:47.790739Z","shell.execute_reply.started":"2025-04-28T11:48:47.784679Z","shell.execute_reply":"2025-04-28T11:48:47.789899Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Function for tokenizing single caption\ndef tokenize_caption(caption, word2idx, max_len=200):\n    tokens = caption.lower().split()\n    token_ids = [word2idx.get(\"<start>\")]\n    for token in tokens:\n        token_ids.append(word2idx.get(token, word2idx[\"<unk>\"]))\n    token_ids.append(word2idx.get(\"<end>\"))\n    \n    if len(token_ids) < max_len:\n        token_ids += [word2idx[\"<pad>\"]] * (max_len - len(token_ids))\n    else:\n        token_ids = token_ids[:max_len]\n    \n    return token_ids\n\n# Tokenize and pad all captions\ncaptions_tokenized = [tokenize_caption(caption, word2idx, max_len=200) for caption in captions]\ncaptions_tensor = torch.tensor(captions_tokenized)\n\ncaptions_tokenized_test = [tokenize_caption(caption, word2idx, max_len=200) for caption in captions_test]\ncaptions_tensor_test = torch.tensor(captions_tokenized_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:47.791385Z","iopub.execute_input":"2025-04-28T11:48:47.791667Z","iopub.status.idle":"2025-04-28T11:48:48.285499Z","shell.execute_reply.started":"2025-04-28T11:48:47.791642Z","shell.execute_reply":"2025-04-28T11:48:48.284947Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Create TensorDatasets for train and test data\ntrain_dataset = TensorDataset(features, captions_tensor)\ntest_dataset = TensorDataset(torch.tensor(features_test), torch.tensor(captions_tensor_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:48.286340Z","iopub.execute_input":"2025-04-28T11:48:48.286625Z","iopub.status.idle":"2025-04-28T11:48:48.291406Z","shell.execute_reply.started":"2025-04-28T11:48:48.286599Z","shell.execute_reply":"2025-04-28T11:48:48.290746Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3871989612.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  test_dataset = TensorDataset(torch.tensor(features_test), torch.tensor(captions_tensor_test))\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# DataLoader for batching\nbatch_size = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:48.292272Z","iopub.execute_input":"2025-04-28T11:48:48.292532Z","iopub.status.idle":"2025-04-28T11:48:48.304325Z","shell.execute_reply.started":"2025-04-28T11:48:48.292508Z","shell.execute_reply":"2025-04-28T11:48:48.303684Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"print(\"Captions tensor min:\", captions_tensor.min())\nprint(\"Captions tensor max:\", captions_tensor.max())\nprint(\"Vocab size:\", vocab_size)\n# captions_tensor.max() should be smaller than vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:48.305077Z","iopub.execute_input":"2025-04-28T11:48:48.305712Z","iopub.status.idle":"2025-04-28T11:48:48.324298Z","shell.execute_reply.started":"2025-04-28T11:48:48.305689Z","shell.execute_reply":"2025-04-28T11:48:48.323758Z"}},"outputs":[{"name":"stdout","text":"Captions tensor min: tensor(0)\nCaptions tensor max: tensor(19413)\nVocab size: 22207\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def print_device(tensor, name=\"Tensor\"):\n    print(f\"{name} is on device: {tensor.device}\")\n\n# Move models to GPU\nencoder_projection = encoder_projection.to(device)\ncaption_embedding = caption_embedding.to(device)\ntransformer_decoder = transformer_decoder.to(device)\noutput_layer = output_layer.to(device)\n\n# Function to run one epoch\ndef run_one_epoch(loader, is_train=True):\n    #traing mode\n    if is_train:\n        encoder_projection.train()\n        caption_embedding.train()\n        transformer_decoder.train()\n        output_layer.train()\n    else: # evaluating mode\n        encoder_projection.eval()\n        caption_embedding.eval()\n        transformer_decoder.eval()\n        output_layer.eval()\n\n    total_loss = 0.0\n\n    for features_batch, captions_batch in loader:\n\n        features_batch = features_batch.to(device)\n        captions_batch = captions_batch.to(device)\n\n        # Define inputs (all tokens except last) and targets (all tokens except first)\n        inputs = captions_batch[:, :-1]\n        targets = captions_batch[:, 1:]\n\n        # Project features and add sequence dimension\n        features_encoded = encoder_projection(features_batch)\n        features_encoded = features_encoded.unsqueeze(1)\n\n        # Embed caption  inputs and add positional encoding\n        captions_embedding = caption_embedding(inputs) \n        captions_embedding = positional_encoding(captions_embedding)\n\n        # Prepare memory\n        memory = features_encoded.permute(1, 0, 2)\n\n        # Move memory to GPU\n        memory = memory.to(device)\n\n        # Prepare target input for decoder\n        tgt = captions_embedding.permute(1, 0, 2)\n\n        # Move target in to GPU\n        tgt = tgt.to(device)\n\n        # Decoder output\n        output = transformer_decoder(tgt=tgt, memory=memory)\n        output = output_layer(output)\n        output = output.permute(1, 0, 2)\n        \n        # Flatten for loss\n        output = output.reshape(-1, vocab_size)\n        targets = targets.reshape(-1)\n\n        loss = criterion(output, targets)\n\n        if is_train:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\n\n# Training process\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    train_loss = run_one_epoch(train_loader, is_train=True)\n    \n    # Save the model if the loss is better\n    if train_loss < best_loss:\n        best_loss = train_loss\n        torch.save({\n            'encoder_projection': encoder_projection.state_dict(),\n            'caption_embedding': caption_embedding.state_dict(),\n            'transformer_decoder': transformer_decoder.state_dict(),\n            'output_layer': output_layer.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'epoch': epoch,\n            }, \"/kaggle/working/best_model.pth\")\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n\n# Test the model\ntest_loss = run_one_epoch(test_loader, is_train=False)\nprint(f\"Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:48:48.325109Z","iopub.execute_input":"2025-04-28T11:48:48.325409Z","iopub.status.idle":"2025-04-28T11:56:49.424204Z","shell.execute_reply.started":"2025-04-28T11:48:48.325368Z","shell.execute_reply":"2025-04-28T11:56:49.423406Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Train Loss: 7.3410\nEpoch [2/10], Train Loss: 6.1477\nEpoch [3/10], Train Loss: 5.4428\nEpoch [4/10], Train Loss: 4.7813\nEpoch [5/10], Train Loss: 4.1733\nEpoch [6/10], Train Loss: 3.5728\nEpoch [7/10], Train Loss: 2.9994\nEpoch [8/10], Train Loss: 2.5115\nEpoch [9/10], Train Loss: 2.1082\nEpoch [10/10], Train Loss: 1.7725\nTest Loss: 2.5277\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"#Save model\nsave_path = \"/kaggle/working/CNN+Transformer_model.pth\"\n\ntorch.save({\n    'encoder_projection': encoder_projection.state_dict(),\n    'caption_embedding': caption_embedding.state_dict(),\n    'transformer_decoder': transformer_decoder.state_dict(),\n    'output_layer': output_layer.state_dict(),\n    'optimizer': optimizer.state_dict(),  # (optional) save optimizer too\n    'epoch': epoch,\n}, save_path)\n\nprint(f\"Model saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:56:49.427293Z","iopub.execute_input":"2025-04-28T11:56:49.427512Z","iopub.status.idle":"2025-04-28T11:56:49.926598Z","shell.execute_reply.started":"2025-04-28T11:56:49.427496Z","shell.execute_reply":"2025-04-28T11:56:49.925791Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/CNN+Transformer_model.pth\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Load the saved model\ncheckpoint = torch.load('/kaggle/working/CNN+Transformer_model.pth', map_location=device)\n\nencoder_projection.load_state_dict(checkpoint['encoder_projection'])\ncaption_embedding.load_state_dict(checkpoint['caption_embedding'])\ntransformer_decoder.load_state_dict(checkpoint['transformer_decoder'])\noutput_layer.load_state_dict(checkpoint['output_layer'])\n\n# Move models to GPU\nencoder_projection = encoder_projection.to(device)\ncaption_embedding = caption_embedding.to(device)\ntransformer_decoder = transformer_decoder.to(device)\noutput_layer = output_layer.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:56:49.927330Z","iopub.execute_input":"2025-04-28T11:56:49.927537Z","iopub.status.idle":"2025-04-28T11:56:50.176452Z","shell.execute_reply.started":"2025-04-28T11:56:49.927520Z","shell.execute_reply":"2025-04-28T11:56:50.175694Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/990657220.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/working/CNN+Transformer_model.pth', map_location=device)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Create index-to-word dictionary\nidx2word = {idx: word for word, idx in word2idx.items()}\n\n# Decode tokes IDs into text\ndef decode_caption(caption_ids):\n    words = []\n    for idx in caption_ids:\n        if idx == word2idx['<end>']:\n            break\n        if idx in (word2idx['<pad>'], word2idx['<start>']):\n            continue\n        words.append(idx2word.get(idx, \"<unk>\"))\n    return ' '.join(words)\n\n# Set the model to evaluation mode\nencoder_projection.eval()\ncaption_embedding.eval()\ntransformer_decoder.eval()\noutput_layer.eval()\n\nreal_captions = []\npredicted_captions = []\n\n# Generate predictions on test set\nwith torch.no_grad():\n    for features_batch, captions_batch in test_loader:\n        features_batch = features_batch.to(device)\n        captions_batch = captions_batch.to(device)\n\n        inputs = captions_batch[:, :-1]\n        targets = captions_batch[:, 1:]\n\n        # Forward pass\n        features_encoded = encoder_projection(features_batch)\n        features_encoded = features_encoded.unsqueeze(1)\n\n        captions_embedding = caption_embedding(inputs)\n        captions_embedding = positional_encoding(captions_embedding)\n\n        memory = features_encoded.permute(1, 0, 2)\n        tgt = captions_embedding.permute(1, 0, 2)\n\n        output = transformer_decoder(tgt=tgt, memory=memory)\n        output = output_layer(output)\n        output = output.permute(1, 0, 2)\n\n        # Get most probable token\n        _, predicted_indices = torch.max(output, dim=-1)\n\n        for i in range(features_batch.size(0)):  # Limit to 5 samples\n            real_caption = decode_caption(captions_batch[i].cpu().tolist())\n            predicted_caption = decode_caption(predicted_indices[i].cpu().tolist())\n\n            real_captions.append(real_caption)\n            predicted_captions.append(predicted_caption)\n\n        break\n\n# Save original and predicted captions\ncaptions_df = pd.DataFrame({\n    'real_caption': real_captions,\n    'predicted_caption': predicted_captions\n})\n\n# Save to CSV file\ncsv_path = \"/kaggle/working/captions.csv\"\ncaptions_df.to_csv(csv_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:56:50.177361Z","iopub.execute_input":"2025-04-28T11:56:50.177644Z","iopub.status.idle":"2025-04-28T11:56:50.282144Z","shell.execute_reply.started":"2025-04-28T11:56:50.177621Z","shell.execute_reply":"2025-04-28T11:56:50.281606Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"captions_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:56:50.282913Z","iopub.execute_input":"2025-04-28T11:56:50.283162Z","iopub.status.idle":"2025-04-28T11:56:50.290328Z","shell.execute_reply.started":"2025-04-28T11:56:50.283138Z","shell.execute_reply":"2025-04-28T11:56:50.289588Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                        real_caption  \\\n0  ct chest axial view showing a huge ascending a...   \n1  computed tomography (ct) shows floating thromb...   \n2  digitally subtracted angiogram demonstrates ac...   \n3  digitally subtracted angiogram of the ima demo...   \n4               angle measurement of a type 1 canal.   \n\n                                   predicted_caption  \n0  ct chest axial view showing a huge ascending a...  \n1  computed tomography (ct) shows thrombus thromb...  \n2  post-operative thorax angiogram demonstrates a...  \n3  post-operative tomographic angiogram of the pa...  \n4                angle measurement of a type 1 month  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>real_caption</th>\n      <th>predicted_caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ct chest axial view showing a huge ascending a...</td>\n      <td>ct chest axial view showing a huge ascending a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>computed tomography (ct) shows floating thromb...</td>\n      <td>computed tomography (ct) shows thrombus thromb...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>digitally subtracted angiogram demonstrates ac...</td>\n      <td>post-operative thorax angiogram demonstrates a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>digitally subtracted angiogram of the ima demo...</td>\n      <td>post-operative tomographic angiogram of the pa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>angle measurement of a type 1 canal.</td>\n      <td>angle measurement of a type 1 month</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# Compute BERTScore\nP, R, F1 = score(predicted_captions, real_captions, lang=\"en\", verbose=True)\n\nprint(f\"Average BERTScore F1: {F1.mean().item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:56:50.291139Z","iopub.execute_input":"2025-04-28T11:56:50.291424Z","iopub.status.idle":"2025-04-28T11:57:16.693703Z","shell.execute_reply.started":"2025-04-28T11:56:50.291404Z","shell.execute_reply":"2025-04-28T11:57:16.693049Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5e355eb0f6c48c693a26f006ec9e55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b051108abd3749f489f98e0ce8de56a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9403713b73df437595daf55def2f409b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cde828565c43fcac989f57251737dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"819ee029bd6d4103916164c1f4c2da7a"}},"metadata":{}},{"name":"stderr","text":"2025-04-28 11:57:01.027667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745841421.188957      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745841421.237070      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10a3d26bda643d5a999cc27f009393b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4060b9d123f2440fa9b1d68247bac756"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef79c9eba6264224a89e87aee51bb976"}},"metadata":{}},{"name":"stdout","text":"done in 0.89 seconds, 71.61 sentences/sec\nAverage BERTScore F1: 0.9355\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"print(f\"BERTScore: Precision={P.mean().item():.4f}, Recall={R.mean().item():.4f}, F1={F1.mean().item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:57:16.694428Z","iopub.execute_input":"2025-04-28T11:57:16.695035Z","iopub.status.idle":"2025-04-28T11:57:16.700915Z","shell.execute_reply.started":"2025-04-28T11:57:16.695014Z","shell.execute_reply":"2025-04-28T11:57:16.699141Z"}},"outputs":[{"name":"stdout","text":"BERTScore: Precision=0.9432, Recall=0.9282, F1=0.9355\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Compute MedBERTScore\nP_med, R_med, F1_med = score(\n    predicted_captions,\n    real_captions,\n    model_type=\"emilyalsentzer/Bio_ClinicalBERT\",\n    num_layers=8,\n    lang=\"en\",\n    rescale_with_baseline=True\n)\nprint(f\"MedBERTScore: Precision={P_med.mean().item():.4f}, Recall={R_med.mean().item():.4f}, F1={F1_med.mean().item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:58:06.177726Z","iopub.execute_input":"2025-04-28T11:58:06.178570Z","iopub.status.idle":"2025-04-28T11:58:12.219323Z","shell.execute_reply.started":"2025-04-28T11:58:06.178519Z","shell.execute_reply":"2025-04-28T11:58:12.218624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"143b91a653fa469bbf46dc690c13c5ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6f494d3a734346ac86cdbd3e3b1f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7abcf8ca28ec4e68bd3d098364a5255a"}},"metadata":{}},{"name":"stdout","text":"MedBERTScore: Precision=0.8713, Recall=0.8234, F1=0.8460\n","output_type":"stream"},{"name":"stderr","text":"Warning: Baseline not Found for emilyalsentzer/Bio_ClinicalBERT on en at /usr/local/lib/python3.11/dist-packages/bert_score/rescale_baseline/en/emilyalsentzer/Bio_ClinicalBERT.tsv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6731578d54d0429c9bffed88a31d9a76"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"# Save results to txt file\nresults = {\n    \"BERTScore Precision\": P.mean().item(),\n    \"BERTScore Recall\": R.mean().item(),\n    \"BERTScore F1\": F1.mean().item(),\n    \"MedBERTScore Precision\": P_med.mean().item(),\n    \"MedBERTScore Recall\": R_med.mean().item(),\n    \"MedBERTScore F1\": F1_med.mean().item(),\n}\n\nresults_path = \"/kaggle/working/results_CNNTransformer.txt\"\n\nwith open(results_path, \"w\") as f:\n    for metric, value in results.items():\n        f.write(f\"{metric}: {value:.4f}\\n\")\n\nprint(f\"Results saved to {results_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T11:59:44.830466Z","iopub.execute_input":"2025-04-28T11:59:44.831112Z","iopub.status.idle":"2025-04-28T11:59:44.837132Z","shell.execute_reply.started":"2025-04-28T11:59:44.831082Z","shell.execute_reply":"2025-04-28T11:59:44.836514Z"}},"outputs":[{"name":"stdout","text":"Results saved to /kaggle/working/results_CNNTransformer.txt\n","output_type":"stream"}],"execution_count":37}]}