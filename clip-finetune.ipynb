{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eirik\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset, load_from_disk, Features, Sequence, Value, Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm # For progress bars\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbac3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b22d8d0e0c844cba830ad9147b0b0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full dataset with 3 samples.\n",
      "Selected 10000 samples for training with columns: ['image', 'image_id', 'caption', 'cui']\n",
      "Selected 4000 samples for validation with columns: ['image', 'image_id', 'caption', 'cui']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Dataset (e.g., ROCOv2 Radiology Dataset)\n",
    "print(\"Loading dataset...\")\n",
    "# Define expected features including CUIs as a sequence of strings\n",
    "# Adjust based on actual dataset structure if needed\n",
    "expected_features = Features({\n",
    "    'image': Image(),\n",
    "    'caption': Value(dtype='string', id=None),\n",
    "    'image_id': Value(dtype='string', id=None),\n",
    "    'cui': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "    # Add other columns if they exist and you might need them\n",
    "})\n",
    "\n",
    "# Using a small subset for faster demonstration/testing\n",
    "train_split_size = 10000 # How many training samples\n",
    "eval_split_size = 4000 # How many validation samples\n",
    "# dataset_name = \"eltorio/ROCOv2-radiology\"\n",
    "dataset_name = \"roco_updated_cuis\"\n",
    "\n",
    "def count_unique_cuis(dataset):\n",
    "    unique_cuis = set()\n",
    "    for sample in dataset:\n",
    "        # print(sample)\n",
    "        if \"cui\" in sample and sample[\"cui\"]:\n",
    "            # print(f\"Sample CUI: {sample['cui']}\")\n",
    "            unique_cuis.update(sample[\"cui\"])  # Add all CUIs in the sample to the set\n",
    "    return len(unique_cuis), unique_cuis\n",
    "\n",
    "try:\n",
    "    full_dataset = load_from_disk(dataset_name)\n",
    "    print(f\"Loaded full dataset with {len(full_dataset)} samples.\")\n",
    "\n",
    "    # Load train split\n",
    "    train_data = full_dataset['train'].select(range(train_split_size))\n",
    "    print(f\"Selected {len(train_data)} samples for training with columns: {train_data.column_names}\")\n",
    "\n",
    "    # Load validation split\n",
    "    validation_data = full_dataset['validation'].select(range(eval_split_size))\n",
    "    print(f\"Selected {len(validation_data)} samples for validation with columns: {validation_data.column_names}\")\n",
    "\n",
    "    # # Count unique CUIs in the training and validation datasets\n",
    "    # train_unique_count, train_unique_cuis = count_unique_cuis(train_data)\n",
    "    # validation_unique_count, validation_unique_cuis = count_unique_cuis(validation_data)\n",
    "\n",
    "    # # Print the results\n",
    "    # print(f\"Number of unique CUIs in the training dataset: {train_unique_count}\")\n",
    "    # print(f\"Number of unique CUIs in the validation dataset: {validation_unique_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(f\"Please ensure the dataset '{dataset_name}' is accessible and has the expected features (image, caption, cui).\")\n",
    "    print(\"You might need to adjust 'expected_features' or the split syntax.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3684f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load CLIP Model and Processor\n",
    "print(\"Loading CLIP model...\")\n",
    "# Use a standard CLIP model checkpoint\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_id)\n",
    "model = CLIPModel.from_pretrained(model_id)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2d9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: CUI to Text Conversion and Data Preparation Function\n",
    "\n",
    "def cuis_to_text(cui_list):\n",
    "    \"\"\"\n",
    "    Converts a list of CUIs into a descriptive text string.\n",
    "    Example: [\"C0040405\", \"C0041618\"] -> \"Medical concepts observed: C0040405, C0041618\"\n",
    "    Handles empty lists.\n",
    "    A more advanced version could map CUIs to names (e.g., using UMLS).\n",
    "    \"\"\"\n",
    "    if not cui_list:\n",
    "        return \"No specific medical concepts identified.\" # Or handle as invalid?\n",
    "    random.shuffle(cui_list)\n",
    "    return \"Medical concepts observed: \" + \", \".join(cui_list)\n",
    "\n",
    "def collate_fn_cui(batch):\n",
    "    valid_batch = []\n",
    "    texts = []\n",
    "\n",
    "    for example in batch:\n",
    "        # Basic validation: Check for image and non-empty CUI list\n",
    "        if example[\"image\"] is not None and example[\"cui\"] and len(example[\"cui\"]) > 0:\n",
    "             valid_batch.append(example)\n",
    "             # Convert CUIs to text representation\n",
    "             texts.append(cuis_to_text(example[\"cui\"]))\n",
    "\n",
    "\n",
    "    if not valid_batch:\n",
    "        return None # Skip batch if no valid examples remain\n",
    "\n",
    "    images = [example[\"image\"] for example in valid_batch]\n",
    "\n",
    "    # Use the CLIP processor\n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, # Pad text sequences to max length in batch\n",
    "        truncation=True # Truncate text sequences if they exceed model max length\n",
    "    )\n",
    "\n",
    "    # Move tensors to the correct device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # Store number of valid items for potential index tracking in evaluation\n",
    "    inputs['num_valid_in_batch'] = len(valid_batch)\n",
    "    return inputs\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_cui)\n",
    "eval_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_cui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afe43ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fine-Tuning Setup\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6) # Smaller LR often better for CLIP FT\n",
    "epochs = 10 # Number of fine-tuning epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a24a0973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cc6395868047d1865bdf8049d46b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Average Training Loss: 0.5419\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9822e9ada44ad79bb7da0855ed2218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Average Training Loss: 0.4048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6952765e014441db8fe15dbe9fce4622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Average Training Loss: 0.3476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b6a654b91540b7b239f7d539705455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Average Training Loss: 0.3250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a505c27594c24fd3aa13f3f64d3110f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Average Training Loss: 0.2867\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fd10eae1f34f20aa4e25a902ce82a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Average Training Loss: 0.2533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc86ba528c042098c27612997c481cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Average Training Loss: 0.2325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee8b9a9acfa42a2bc554496718767f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Average Training Loss: 0.2095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a7fa0824e24421a503e0d84ec45631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Average Training Loss: 0.1960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3def82b91734b6fad762106133cd9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Average Training Loss: 0.1783\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Fine-Tuning Loop\n",
    "print(\"Starting fine-tuning...\")\n",
    "model.train() # Set model to training mode\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches_processed = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        if batch is None: # Skip if collate_fn returned None\n",
    "            print(\"Skipping empty batch.\")\n",
    "            continue\n",
    "\n",
    "        # Extract valid count and remove it from batch dict before passing to model\n",
    "        num_valid = batch.pop('num_valid_in_batch', None)\n",
    "        if num_valid is None or num_valid == 0: # Should not happen if batch is not None, but check\n",
    "             continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: CLIPModel calculates contrastive loss\n",
    "        outputs = model(**batch, return_loss=True)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches_processed += 1\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "    if num_batches_processed > 0:\n",
    "        avg_loss = total_loss / num_batches_processed\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Training Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed with no valid batches processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba752ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed and model saved to ./clip-finetuned-medical-cui-10000-10\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save Fine-Tuned Model\n",
    "save_directory = \"./clip-finetuned-medical-cui-10000-10\"\n",
    "model.save_pretrained(save_directory)\n",
    "processor.save_pretrained(save_directory)\n",
    "print(f\"Fine-tuning completed and model saved to {save_directory}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
