{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d8da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansatt/eriksh/.venv/dat550/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"eltorio/ROCOv2-radiology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512523c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b564a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68a0a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54523ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'image_id', 'caption', 'cui'],\n",
      "        num_rows: 59962\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'image_id', 'caption', 'cui'],\n",
      "        num_rows: 9904\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'image_id', 'caption', 'cui'],\n",
      "        num_rows: 9927\n",
      "    })\n",
      "})\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=682x748 at 0x7F2198845F00>, 'image_id': 'ROCOv2_2023_train_000001', 'caption': 'Head CT demonstrating left parotiditis.', 'cui': ['C0040405']}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset structure\n",
    "print(ds)\n",
    "print(ds['train'][0])  # Print the first example in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5324da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Head CT demonstrating left parotiditis.', 'Acquired renal cysts in end-stage renal failure: 16-year-old girl with Alport syndrome and peritoneal dialysis from the age of 2\\xa0years', 'Computed tomography of the chest showing the right breast nodule with irregular margins', 'Lateral view of the sacrum showing the low contrast between bone and soft tissue.', 'Thoracic CT scan showing perihilar pulmonary lymphadenomegaly']\n"
     ]
    }
   ],
   "source": [
    "# Extract images and captions from the dataset\n",
    "images = [example[\"image\"] for example in ds[\"train\"]]\n",
    "captions = [example[\"caption\"] for example in ds[\"train\"]]\n",
    "\n",
    "# Print the first 5 captions to verify\n",
    "print(captions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba1e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<GPUtil.GPUtil.GPU object at 0x7f2198845ea0>, <GPUtil.GPUtil.GPU object at 0x7f1e86e59480>, <GPUtil.GPUtil.GPU object at 0x7f2198845de0>, <GPUtil.GPUtil.GPU object at 0x7f1e86cbc910>, <GPUtil.GPUtil.GPU object at 0x7f1e86cbc490>, <GPUtil.GPUtil.GPU object at 0x7f1e86cbc4f0>]\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import psutil\n",
    "import GPUtil\n",
    "print(GPUtil.getGPUs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f1fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ID: 0\n",
      "Name: Tesla P100-PCIE-12GB\n",
      "Load: 0.00%\n",
      "Memory Free: 13.0MB\n",
      "Memory Used: 12180.0MB\n",
      "Memory Total: 12288.0MB\n",
      "Temperature: 34.0°C\n",
      "----------------------------------------\n",
      "GPU ID: 1\n",
      "Name: Tesla P100-PCIE-16GB\n",
      "Load: 0.00%\n",
      "Memory Free: 16274.0MB\n",
      "Memory Used: 2.0MB\n",
      "Memory Total: 16384.0MB\n",
      "Temperature: 38.0°C\n",
      "----------------------------------------\n",
      "GPU ID: 2\n",
      "Name: Tesla P100-PCIE-12GB\n",
      "Load: 0.00%\n",
      "Memory Free: 12191.0MB\n",
      "Memory Used: 2.0MB\n",
      "Memory Total: 12288.0MB\n",
      "Temperature: 36.0°C\n",
      "----------------------------------------\n",
      "GPU ID: 3\n",
      "Name: Tesla P100-PCIE-12GB\n",
      "Load: 0.00%\n",
      "Memory Free: 12191.0MB\n",
      "Memory Used: 2.0MB\n",
      "Memory Total: 12288.0MB\n",
      "Temperature: 31.0°C\n",
      "----------------------------------------\n",
      "GPU ID: 4\n",
      "Name: Tesla P100-PCIE-16GB\n",
      "Load: 0.00%\n",
      "Memory Free: 16274.0MB\n",
      "Memory Used: 2.0MB\n",
      "Memory Total: 16384.0MB\n",
      "Temperature: 31.0°C\n",
      "----------------------------------------\n",
      "GPU ID: 5\n",
      "Name: Tesla P100-PCIE-16GB\n",
      "Load: 0.00%\n",
      "Memory Free: 16274.0MB\n",
      "Memory Used: 2.0MB\n",
      "Memory Total: 16384.0MB\n",
      "Temperature: 33.0°C\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all available GPUs\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "# Print details for each GPU\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU ID: {gpu.id}\")\n",
    "    print(f\"Name: {gpu.name}\")\n",
    "    print(f\"Load: {gpu.load * 100:.2f}%\")\n",
    "    print(f\"Memory Free: {gpu.memoryFree}MB\")\n",
    "    print(f\"Memory Used: {gpu.memoryUsed}MB\")\n",
    "    print(f\"Memory Total: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Temperature: {gpu.temperature}°C\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96bce5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set GPU ID 4 as the visible device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a147309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f454766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmedtekki\u001b[0m (\u001b[33mmedtekki-university-of-stavanger\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/beegfs/home/eriksh/DAT550/project/wandb/run-20250422_104037-pr9gob5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/pr9gob5i' target=\"_blank\">feature-extraction_testrun2</a></strong> to <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/pr9gob5i' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/pr9gob5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansatt/eriksh/.venv/dat550/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ansatt/eriksh/.venv/dat550/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and captions saved locally.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>image_index</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>caption</td><td>Right shoulder of a ...</td></tr><tr><td>image_index</td><td>59961</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feature-extraction_testrun2</strong> at: <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/pr9gob5i' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/pr9gob5i</a><br> View project at: <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning</a><br>Synced 5 W&B file(s), 59962 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250422_104037-pr9gob5i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.init(project=\"medical-image-captioning\", name=\"feature-extraction_testrun2\")\n",
    "\n",
    "# Log configuration\n",
    "wandb.config.update({\n",
    "    \"model\": \"ResNet50\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"image_size\": (224, 224),\n",
    "    \"normalization_mean\": [0.485, 0.456, 0.406],\n",
    "    \"normalization_std\": [0.229, 0.224, 0.225]\n",
    "})\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load a pre-trained ResNet model and move it to the GPU\n",
    "resnet = models.resnet50(pretrained=True).to(device)\n",
    "resnet.eval()\n",
    "\n",
    "# Define a transformation pipeline for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image):\n",
    "    # Ensure the image is in RGB format\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Apply transformations\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "    \n",
    "    # Extract features using the model\n",
    "    with torch.no_grad():\n",
    "        features = resnet(image_tensor)\n",
    "    \n",
    "    return features.squeeze().cpu().numpy()  # Move back to CPU for further processing\n",
    "\n",
    "# Initialize lists to store features and captions\n",
    "all_features = []\n",
    "all_captions = []\n",
    "\n",
    "# Extract features for all images and log to wandb\n",
    "for idx, (image, caption) in enumerate(zip(images, captions)):\n",
    "    features = extract_features(image)\n",
    "    all_features.append(features)\n",
    "    all_captions.append(caption)\n",
    "    \n",
    "    # Log features and captions to wandb\n",
    "    wandb.log({\n",
    "        \"image_index\": idx,\n",
    "        \"caption\": caption,\n",
    "        \"features\": features.tolist(),  # Log features as a list\n",
    "        \"image\": wandb.Image(image, caption=caption)\n",
    "    })\n",
    "\n",
    "# Save features and captions to local files\n",
    "np.save(\"features.npy\", np.array(all_features))  # Save features as a NumPy array\n",
    "with open(\"captions.json\", \"w\") as f:\n",
    "    json.dump(all_captions, f)  # Save captions as a JSON file\n",
    "\n",
    "print(\"Features and captions saved locally.\")\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c724d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59dfaa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Name: feature-extraction, Run ID: stdmmc54\n",
      "Run Name: feature-extraction2, Run ID: htt1q4t7\n",
      "Run Name: feature-extraction3, Run ID: pon7o8gv\n",
      "Run Name: feature-extraction_ichi, Run ID: sgqh7c5b\n",
      "Run Name: feature-extraction_ni, Run ID: tcm61qyd\n",
      "Run Name: feature-extraction_ni, Run ID: zp4419tw\n",
      "Run Name: feature-extraction_san, Run ID: so58vuk3\n",
      "Run Name: feature-extraction_ni, Run ID: y8i2dwf9\n",
      "Run Name: feature-extraction_chi, Run ID: lity27dr\n",
      "Run Name: feature-extraction_go, Run ID: ompi5td7\n",
      "Run Name: feature-extraction_testrun, Run ID: 6lunze22\n",
      "Run Name: feature-extraction_testrun2, Run ID: pr9gob5i\n",
      "Run Name: decision-tree-mps, Run ID: k1a5rxte\n",
      "Run Name: decision-tree-mps_ichi, Run ID: s3yvrg1s\n",
      "Run Name: decision-tree-mps_ni, Run ID: y2dojsyl\n"
     ]
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "runs = api.runs(\"medtekki-university-of-stavanger/medical-image-captioning\")\n",
    "for run in runs:\n",
    "    print(f\"Run Name: {run.name}, Run ID: {run.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fccd6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_runtime\n",
      "features\n",
      "_timestamp\n",
      "image_index\n",
      "_step\n",
      "image\n",
      "caption\n"
     ]
    }
   ],
   "source": [
    "run = api.run(f\"medtekki-university-of-stavanger/medical-image-captioning/sgqh7c5b\")\n",
    "for log in run.history():\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463ec157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f296b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/beegfs/home/eriksh/DAT550/project/wandb/run-20250422_195236-1ber5z16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/1ber5z16' target=\"_blank\">decision-tree-classifier_go4</a></strong> to <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/1ber5z16' target=\"_blank\">https://wandb.ai/medtekki-university-of-stavanger/medical-image-captioning/runs/1ber5z16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 477192 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type list that is 499960 bytes\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"medical-image-captioning\", name=\"decision-tree-classifier_go4\")\n",
    "\n",
    "# Load the saved features and captions\n",
    "features = np.load(\"features.npy\")  # Load features from the .npy file\n",
    "with open(\"captions.json\", \"r\") as f:\n",
    "    captions = json.load(f)  # Load captions from the .json file\n",
    "\n",
    "# Step 1: Convert captions to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(captions)\n",
    "\n",
    "# Log label classes\n",
    "wandb.config.label_classes = label_encoder.classes_.tolist()\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the Classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "\n",
    "# Log metrics to W&B\n",
    "wandb.log({\"accuracy\": accuracy, \"classification_report\": report})\n",
    "\n",
    "# Step 5: Log confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=label_encoder.classes_, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# Optional: Save and log the trained model\n",
    "joblib.dump(clf, \"decision_tree_model.pkl\")\n",
    "wandb.save(\"decision_tree_model.pkl\")\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada7026",
   "metadata": {},
   "source": [
    "## TF-IDF + KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78270016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of captions loaded: 59962\n",
      "First 5 captions: ['Head CT demonstrating left parotiditis.', 'Acquired renal cysts in end-stage renal failure: 16-year-old girl with Alport syndrome and peritoneal dialysis from the age of 2\\xa0years', 'Computed tomography of the chest showing the right breast nodule with irregular margins', 'Lateral view of the sacrum showing the low contrast between bone and soft tissue.', 'Thoracic CT scan showing perihilar pulmonary lymphadenomegaly']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# Load the saved features and captions\n",
    "features = np.load(\"features.npy\")  # Load features from the .npy file\n",
    "with open(\"captions.json\", \"r\") as f:\n",
    "    captions = json.load(f)  # Load captions from the .json file\n",
    "\n",
    "# Verify the loaded captions\n",
    "print(f\"Number of captions loaded: {len(captions)}\")\n",
    "print(\"First 5 captions:\", captions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820fe409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Convert captions to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X = vectorizer.fit_transform(captions)\n",
    "\n",
    "# Cluster captions into k groups\n",
    "k = 30  # Choose the number of clusters (adjust based on your dataset)\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Map captions to their clusters\n",
    "clustered_captions = {caption: cluster for caption, cluster in zip(captions, clusters)}\n",
    "\n",
    "# Replace captions with cluster labels\n",
    "labels = clusters\n",
    "print(f\"Number of clusters: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de172d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, \"decision_tree_model.pkl\")\n",
    "print(\"Model saved as decision_tree_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8687d6",
   "metadata": {},
   "source": [
    "Step 3: Set Up LLaVA\n",
    "LLaVA requires a model checkpoint. You can use the ollama library to download and run the model locally.\n",
    "\n",
    "Download the LLaVA Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16224ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama pull llava"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a06ecb",
   "metadata": {},
   "source": [
    "Start the LLaVA Server: Run the following command to start the LLaVA server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b145a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa2ce0",
   "metadata": {},
   "source": [
    "Step 4: Predict Captions for Images\n",
    "Use the ollama Python API to send image data to the LLaVA model and get captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1fdc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc848f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb5cca9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat550",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
